---
title: "North Carolina Education and Duke Students' Impact"
subtitle: "STA 325 ML and Data Mining Project Proposal"
author: "Ethan Shen, Steven Herrera, Malavi Ravindran, Austin Jia"
date: "`r format(Sys.time(), '%B %d %Y')`"
output: rmdformats::readthedown
---
 
## Packages 

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(rvest)
library(knitr)
library(cowplot)
```

***

# Executive Summary

According to US News' Education Rankings, North Carolina ranks **12th** for higher education and **28th** for Pre-K-12 education. These rankings are determined by a number of factors, including data on the percentage of adults with associate degrees or higher, the rates of students completing public 4-year and 2-year college programs, and average tuition for colleges, being some of those factors, while, for K-12 programs, the process of determing ranks is even more complex. Some of these factors include statistics such as high school graduation rate, preschool enrollment, standardized test scores in math and reading, and college readiness based on SAT and ACT scores. While the rankings are flawed with subjective bias, we seek to understand the K-12 education pipeline among several school districts in NC and relationships found among exploring what factors influence the ability to perform well on standardized tests. 

## Key Project Objectives 

Our objectives are as follow: **(1)** develop a model to understand inference (and prioritize interpretability) among schools that score high on the SAT, ACT, and NC school growth index; **(2)** complete K-fold cross-validation to determine the best model; and **(3)** predict and interpret whether there is a significant difference between binary explanatory variables using bootstrap simulations.

## High-level Summary of Project Plans

We will consider developing several models to predict SAT, ACT, and NC school growth index scores, each individually, and try to make sense of what variables that we use to predict these response variables are useful in understanding the expected outcomes. Additionally, we will use traditional methods, such as K-fold cross validation and test MSE to choose the best model. Once we conclude which model works the best, we will finalize our results and work to understand how this fits into the greater context of Duke students in North Carolina.

## Importance of this Project and Who Can Benefit From This

As students that attend a private research university in North Carolina, a state with a rich and long history of putting education first (with key political figures like former Governor and US Senator Terry Sanford) and institutionalizing these accomplishments (with the development of eponymous associations like the Research Triangle), we believe making steps towards understanding the larger sociopolitical nuances of education data research, contextualizing this information, and disseminating it to our peers could make huge strides in contributing to local school district efforts to reach higher educational outcomes. We pride ourselves on being students that care about being leaders of the community, but we often fail to understand where those additional resources need to be allocated or what tangible efforts we can make to close the education gap. Nelson Mandela once said "Education is the most powerful weapon which you can use to change the world." If we can dedicate our knowledge of machine learning and data mining to something as important as education (especially in a state that does so much for us) by investing in our future, then we can begin to have these important conversations about our world's most difficult issues to solve, such as climate change. Conducting our statistical analysis could bridge communication among Duke students to serve certain areas of NC through summer research experiences, education policy fellowships, and/or non-profit organization during-semester volunteering.

## Other Relevant Information 

Although hefty of a goal, we do aim to specify our problem much more as we explore new relationships in our final presentation. This proposal is created to show what our initial thoughts were and where we can go with this information.

***

# Data Description 

## Data Sources

After considering many resources, we decided to reach out to the Carolina Population Center at UNC, in which we expressed our interests in data related to NC. Although we scheduled a meeting with them on November 15, we won't have enough time to fully go over the data sources necessary for the type of research we mentioned above in a timely manner, so we decided to just consider many things and debrief with them our findings. Here is the link for the publicly available data: [NC Public Schools Analysis and Reporting](http://www.ncpublicschools.org/accountability/reporting/). We will also be working with data only from the 2018-2019 school year, and, specifically, ACT data from the 2018 11th grade scores and the [2018 SAT scores in NC](http://www.ncpublicschools.org/accountability/reporting/sat/).
 
## Predictors and Response

Here is an outline for each of our datasets and the model variable types we would explore:

  1. SAT
    - Response: `satScoreTotal`
  
  2. ACT
    - Response: `benchmarkPercent`
  
  3. NC School Growth Index
    - Response: `indexScore`
    
While we haven't yet joined our datasets, we plan on heavily using the `percentTested` and `numberTested` variables from the **SAT** dataset, the `region` and `categories` variables in the **ACT** dataset (`hurricaneDays` also shared with the Met dataset), and the `growthStatus` and `growthMet` variables from the **Met** dataset. Additionally, we would ideally combine data from the SAT and ACT datasets to focus on predicting the NC school growth index score, as this is considered the most important predictor of school success since the past year.
 
## Data Scraping / Wrangling 

```{r warning = FALSE, message = FALSE, echo = FALSE}
sat_2018 <- read_csv("sat_2018.csv")

sat <- sat_2018 %>%
  mutate(school = `...2`,
         percentTested = as.double(`% Tested`),
         numberTested = as.numeric(`# Tested`),
         satScoreTotal = as.numeric(`Total`),
         satEnglishScore = as.numeric(`ERW`),
         satMathScore = as.numeric(`Math`)) %>%
  select(school, percentTested, numberTested,
         satScoreTotal, satEnglishScore, satMathScore) %>%
  filter(!is.na(school))
```

```{r warning = FALSE, message = FALSE, echo = FALSE}
met_standards <- read_csv("met_standards.csv")

met <- met_standards %>%
  mutate(district = `District Name`,
         schoolCode = `School Code`,
         school = `School Name`,
         grades = as.factor(`Grade Span`),
         hurricaneDays = as.double(`Missed Days due to Hurricane Florence`),
         growthType = as.factor(`School Growth Type`),
         growthStatus = as.factor(`School Growth Status`),
         indexScore = `School Growth Index Score`) %>%
  select(district, schoolCode, school, grades, hurricaneDays, 
         growthType, growthStatus, indexScore)
```

```{r warning = FALSE, message = FALSE, echo = FALSE}
act_data <- read_csv("act_data.csv")

act <- act_data %>%
  mutate(district = `District Name`,
         schoolCode = `School\r\nCode`,
         school = `School Name`,
         region = as.factor(`State Board Region`),
         grades = as.factor(`Grade Span`),
         hurricaneDays = as.double(`Missed Days due to Hurricane Florence`),
         categories = as.factor(Subgroup),
         actType = as.factor(`ACT Subtest or Composite`),
         benchmarkPercent = as.double(`Percent Meeting Benchmark or Standard`)) %>%
  select(district, schoolCode, school, region,
         grades, hurricaneDays, categories, actType, benchmarkPercent)
```

Below, are just small glimpses of our three datasets we'll be using: `sat`, `act`, and `met`. We did a lot of wrangling to get the unstructured data to format neatly, and, with more time, will plan to join some of these datasets strategically, since they all share some variable names together.

```{r}
kable(sat %>% head(2))
kable(act %>% head(2))
kable(met %>% head(2))
```

## Data Type 

Here, we laid out the data types for all variables in each dataset:

```{r}
kable(sapply(sat, typeof), format = "html", col.names = c("Type"))
kable(sapply(act, typeof), format = "html", col.names = c("Type"))
kable(sapply(met, typeof), format = "html", col.names = c("Type"))
```


*** 

# Project Roadmap 

## Inference and Prediction Goals

In order to meet our objectives, we will follow these steps:

  1. Complete necessary exploratory data analysis to understand key variables in our model to predict SAT, ACT, and NC school growth index score, each.
 
  2. Fit several models, including, but not limited to, linear regression, ridge regression, polynomial regression, GAM regression, and splines.
  
  3. Conduct K-fold cross validation and output important values, such as AIC and test MSE, to choose the best model.
  
  4. Conclude which model works the best and put together a summary of the findings.
  
  5. Finalize thoughts on whether there are statistically significant differences among binary random variables in our dataset (i.e. rural vs. urban, small vs. large school size, etc).
  
In addition to these steps, we've demonstrated a bit of introductory exploratory data analysis:

```{r warning = FALSE}
title <- ggdraw() + 
  draw_label("SAT Scores by Total and Type") +
  theme(plot.margin = margin(0, 0, 0, 7))

b1 <- ggplot(sat, mapping = aes(x = satScoreTotal)) +
  geom_histogram(bins = 30) +
  labs(title = "Total SAT Score in NC",
      x = "SAT Scores")
b2 <- ggplot(sat, mapping = aes(x = satMathScore)) +
  geom_histogram(bins = 30) +
  labs(title = "Math SAT Score in NC",
      x = "Math Scores")
b3 <- ggplot(sat, mapping = aes(x = satEnglishScore)) +
  geom_histogram(bins = 30) +
  labs(title = "English SAT Score in NC",
      x = "English Scores")

plot_grid(title, plot_grid(b1, b2, b3),
          ncol = 1, 
          rel_heights = c(0.1,1))
```


```{r}
act %>%
  count(actType)
```


```{r warning = FALSE}

title2 <- ggdraw() + 
  draw_label("ACT Scores by Total and Type") +
  theme(plot.margin = margin(0, 0, 0, 7))

f1 <- ggplot(act %>% filter(actType == "ACT composite score of 17 or higher"), mapping = aes(x = benchmarkPercent)) +
  geom_histogram(bins = 30) +
  labs(title = "ACT composite score")
f2 <- ggplot(act %>% filter(actType == "ACT English subtest"), mapping = aes(x = benchmarkPercent)) +
  geom_histogram(bins = 30) +
  labs(title = "English subtest")
f3 <- ggplot(act %>% filter(actType == "ACT Math subtest"), mapping = aes(x = benchmarkPercent)) +
  geom_histogram(bins = 30) +
  labs(title = "Math Subtest")
f4 <- ggplot(act %>% filter(actType == "ACT Reading subtest"), mapping = aes(x = benchmarkPercent)) +
  geom_histogram(bins = 30) +
  labs(title = "Reading subtest")
f5 <- ggplot(act %>% filter(actType == "ACT Science subtest"), mapping = aes(x = benchmarkPercent)) +
  geom_histogram(bins = 30) +
  labs(title = "Science subtest")
f6 <- ggplot(act %>% filter(actType == "ACT Writing subtest"), mapping = aes(x = benchmarkPercent)) +
  geom_histogram(bins = 30) +
  labs(title = "Writing subtest")

plot_grid(title2, plot_grid(f1, f2, f3, f4, f5, f6),
          ncol = 1, 
          rel_heights = c(0.1,1))
```



## Modeling Strategies

Referring back to Step 2, we think testing for all of these types of models are good, especially as we consider interpretation to be super important in our entire analysis. Thus, we eliminated models such as random forest modeling, since they are used more for predictability purposes. 

Once we finalize which models work best, by comparing several things (i.e. how do transformations play into the grander scheme of modeling?), we will use key methods in model selection from our labs, explained in our final section.

## Selecting Good Model

We will use the traditional method of considering K-fold cross validation with several folds, since we can account for that with our large datasets, and compare between the models using AIC and test MSE.
